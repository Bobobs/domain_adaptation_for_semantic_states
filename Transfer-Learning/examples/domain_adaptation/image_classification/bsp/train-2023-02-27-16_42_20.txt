Namespace(arch='convnext_base', batch_size=32, bottleneck_dim=256, data='Office31', epochs=20, iters_per_epoch=1000, log='bsp', lr=0.003, lr_decay=0.75, lr_gamma=0.001, momentum=0.9, no_hflip=False, no_pool=False, norm_mean=(0.485, 0.456, 0.406), norm_std=(0.229, 0.224, 0.225), per_class_eval=False, phase='train', pretrain=None, pretrain_epochs=3, pretrain_lr=0.001, print_freq=100, ratio=[0.75, 1.3333333333333333], resize_size=224, root='data/office31', scale=[0.08, 1.0], scratch=False, seed=None, source=['A'], target=['W'], trade_off=1.0, trade_off_bsp=0.0002, train_resizing='default', val_resizing='default', weight_decay=0.001, workers=2)
=> using model 'convnext_base'
Pretraining the model on source domain.
/home/gtzelepis/miniconda3/envs/domain_adaptation/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
lr: [0.0001, 0.001, 0.001]
Epoch: [0][   0/1000]	Time 3.0 (3.0)	Data 0.0 (0.0)	Loss 2.44 (2.44)	Cls Acc 3.1 (3.1)
Epoch: [0][ 100/1000]	Time 0.6 (0.7)	Data 0.0 (0.2)	Loss 1.11 (1.53)	Cls Acc 59.4 (48.3)
Epoch: [0][ 200/1000]	Time 0.8 (0.7)	Data 0.3 (0.2)	Loss 0.89 (1.30)	Cls Acc 65.6 (55.7)
Epoch: [0][ 300/1000]	Time 0.6 (0.7)	Data 0.0 (0.2)	Loss 0.93 (1.18)	Cls Acc 62.5 (59.8)
Epoch: [0][ 400/1000]	Time 0.9 (0.7)	Data 0.3 (0.2)	Loss 0.72 (1.10)	Cls Acc 71.9 (62.5)
Epoch: [0][ 500/1000]	Time 0.8 (0.7)	Data 0.3 (0.2)	Loss 0.89 (1.05)	Cls Acc 71.9 (64.2)
Epoch: [0][ 600/1000]	Time 0.9 (0.7)	Data 0.3 (0.2)	Loss 0.74 (1.00)	Cls Acc 71.9 (65.7)
Epoch: [0][ 700/1000]	Time 0.9 (0.7)	Data 0.3 (0.2)	Loss 0.67 (0.97)	Cls Acc 68.8 (66.8)
Epoch: [0][ 800/1000]	Time 0.9 (0.7)	Data 0.3 (0.2)	Loss 0.44 (0.94)	Cls Acc 84.4 (67.7)
Epoch: [0][ 900/1000]	Time 0.9 (0.7)	Data 0.3 (0.2)	Loss 0.86 (0.91)	Cls Acc 65.6 (68.5)
/home/gtzelepis/Transfer-Learning-Library/examples/domain_adaptation/image_classification/utils.py:136: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  top2, predicted2 = torch.topk(sm(output.data), 2, dim=1, largest=True, sorted=True)
/home/gtzelepis/Transfer-Learning-Library/examples/domain_adaptation/image_classification/utils.py:137: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  top3, predicted3 = torch.topk(sm(output.data), 3, dim=1, largest=True, sorted=True)
/home/gtzelepis/Transfer-Learning-Library/examples/domain_adaptation/image_classification/utils.py:138: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  top9, predicted9 = torch.topk(sm(output.data), 9, dim=1, largest=True, sorted=True)
Test: [  0/295]	Time  1.207 ( 1.207)	Loss 4.5132e-01 (4.5132e-01)	Acc@1  93.75 ( 93.75)
Test: [100/295]	Time  0.358 ( 0.367)	Loss 3.4173e-01 (3.7464e-01)	Acc@1  93.75 ( 91.52)
Test: [200/295]	Time  0.125 ( 0.362)	Loss 3.1588e-01 (3.6507e-01)	Acc@1  87.50 ( 91.82)
 * Acc@1 91.902
lr: [5.946035575013605e-05, 0.0005946035575013605, 0.0005946035575013605]
Epoch: [1][   0/1000]	Time 0.6 (0.6)	Data 0.0 (0.0)	Loss 0.55 (0.55)	Cls Acc 84.4 (84.4)
Epoch: [1][ 100/1000]	Time 0.6 (0.7)	Data 0.0 (0.1)	Loss 0.68 (0.64)	Cls Acc 81.2 (77.9)
Epoch: [1][ 200/1000]	Time 0.8 (0.7)	Data 0.2 (0.1)	Loss 0.50 (0.64)	Cls Acc 87.5 (77.5)
Epoch: [1][ 300/1000]	Time 0.8 (0.7)	Data 0.2 (0.1)	Loss 0.44 (0.64)	Cls Acc 90.6 (77.2)
Epoch: [1][ 400/1000]	Time 0.8 (0.7)	Data 0.2 (0.1)	Loss 0.70 (0.64)	Cls Acc 78.1 (77.2)
Epoch: [1][ 500/1000]	Time 0.6 (0.7)	Data 0.0 (0.1)	Loss 0.53 (0.64)	Cls Acc 84.4 (77.2)
Epoch: [1][ 600/1000]	Time 0.6 (0.7)	Data 0.0 (0.1)	Loss 0.60 (0.63)	Cls Acc 75.0 (77.3)
Epoch: [1][ 700/1000]	Time 0.6 (0.7)	Data 0.0 (0.1)	Loss 0.66 (0.63)	Cls Acc 75.0 (77.3)
Epoch: [1][ 800/1000]	Time 0.8 (0.7)	Data 0.3 (0.1)	Loss 0.60 (0.63)	Cls Acc 78.1 (77.4)
Epoch: [1][ 900/1000]	Time 0.8 (0.7)	Data 0.2 (0.1)	Loss 0.64 (0.62)	Cls Acc 71.9 (77.6)
Test: [  0/295]	Time  1.104 ( 1.104)	Loss 3.6514e-01 (3.6514e-01)	Acc@1  93.75 ( 93.75)
Test: [100/295]	Time  0.270 ( 0.367)	Loss 3.1515e-01 (2.8571e-01)	Acc@1  87.50 ( 93.75)
Test: [200/295]	Time  0.208 ( 0.361)	Loss 2.2525e-01 (2.7922e-01)	Acc@1 100.00 ( 93.66)
 * Acc@1 93.645
lr: [4.3869133765083086e-05, 0.0004386913376508308, 0.0004386913376508308]
Epoch: [2][   0/1000]	Time 0.6 (0.6)	Data 0.0 (0.0)	Loss 0.39 (0.39)	Cls Acc 90.6 (90.6)
Epoch: [2][ 100/1000]	Time 0.6 (0.7)	Data 0.0 (0.1)	Loss 0.62 (0.56)	Cls Acc 68.8 (79.0)
Epoch: [2][ 200/1000]	Time 0.6 (0.7)	Data 0.0 (0.1)	Loss 0.49 (0.58)	Cls Acc 81.2 (79.0)
Epoch: [2][ 300/1000]	Time 0.6 (0.7)	Data 0.0 (0.1)	Loss 0.45 (0.57)	Cls Acc 87.5 (79.2)
Epoch: [2][ 400/1000]	Time 0.9 (0.7)	Data 0.3 (0.1)	Loss 0.48 (0.57)	Cls Acc 81.2 (79.3)
Epoch: [2][ 500/1000]	Time 0.9 (0.7)	Data 0.3 (0.1)	Loss 0.65 (0.57)	Cls Acc 81.2 (78.9)
Epoch: [2][ 600/1000]	Time 0.8 (0.7)	Data 0.3 (0.1)	Loss 0.62 (0.57)	Cls Acc 75.0 (79.2)
Epoch: [2][ 700/1000]	Time 0.9 (0.7)	Data 0.3 (0.1)	Loss 0.65 (0.57)	Cls Acc 84.4 (79.1)
Epoch: [2][ 800/1000]	Time 0.8 (0.7)	Data 0.3 (0.1)	Loss 0.74 (0.57)	Cls Acc 75.0 (79.0)
Epoch: [2][ 900/1000]	Time 1.0 (0.7)	Data 0.4 (0.1)	Loss 0.42 (0.57)	Cls Acc 90.6 (79.0)
Test: [  0/295]	Time  0.975 ( 0.975)	Loss 3.0802e-01 (3.0802e-01)	Acc@1  87.50 ( 87.50)
Test: [100/295]	Time  0.454 ( 0.364)	Loss 3.0513e-01 (2.5151e-01)	Acc@1  87.50 ( 93.94)
Test: [200/295]	Time  0.205 ( 0.357)	Loss 1.7573e-01 (2.4728e-01)	Acc@1 100.00 ( 94.12)
 * Acc@1 93.815
Pretraining process is done.
after pretraining lr: 0.00030000000000000003
Epoch: [0][   0/1000]	Time  3.47 ( 3.47)	Data  0.01 ( 0.01)	Loss   2.09 (  2.09)	Cls Acc 75.0 (75.0)	Domain Acc 60.9 (60.9)
Epoch: [0][ 100/1000]	Time  1.29 ( 1.30)	Data  0.02 ( 0.02)	Loss   1.35 (  1.58)	Cls Acc 84.4 (77.5)	Domain Acc 87.5 (83.4)
Epoch: [0][ 200/1000]	Time  1.28 ( 1.29)	Data  0.02 ( 0.02)	Loss   0.94 (  1.41)	Cls Acc 84.4 (77.4)	Domain Acc 100.0 (89.8)
Epoch: [0][ 300/1000]	Time  1.29 ( 1.29)	Data  0.02 ( 0.02)	Loss   1.05 (  1.32)	Cls Acc 62.5 (77.4)	Domain Acc 100.0 (92.7)
Epoch: [0][ 400/1000]	Time  1.29 ( 1.29)	Data  0.02 ( 0.02)	Loss   1.05 (  1.25)	Cls Acc 84.4 (78.0)	Domain Acc 96.9 (93.9)
Epoch: [0][ 500/1000]	Time  1.29 ( 1.29)	Data  0.02 ( 0.02)	Loss   1.79 (  1.27)	Cls Acc 87.5 (78.4)	Domain Acc 67.2 (92.2)
Epoch: [0][ 600/1000]	Time  1.28 ( 1.29)	Data  0.02 ( 0.02)	Loss   2.63 (  1.58)	Cls Acc 68.8 (77.4)	Domain Acc 70.3 (87.3)
Epoch: [0][ 700/1000]	Time  1.28 ( 1.29)	Data  0.02 ( 0.02)	Loss   1.80 (  1.89)	Cls Acc 78.1 (76.6)	Domain Acc 65.6 (84.0)
Epoch: [0][ 800/1000]	Time  1.29 ( 1.29)	Data  0.02 ( 0.02)	Loss   1.27 (  1.89)	Cls Acc 78.1 (76.3)	Domain Acc 96.9 (82.4)
Epoch: [0][ 900/1000]	Time  1.28 ( 1.29)	Data  0.02 ( 0.02)	Loss   1.53 (  1.86)	Cls Acc 84.4 (76.2)	Domain Acc 84.4 (81.8)
Test: [  0/295]	Time  1.031 ( 1.031)	Loss 4.7672e+00 (4.7672e+00)	Acc@1  18.75 ( 18.75)
Test: [100/295]	Time  0.439 ( 0.364)	Loss 2.8086e+00 (3.1288e+00)	Acc@1  37.50 ( 36.01)
Test: [200/295]	Time  0.584 ( 0.360)	Loss 3.1356e+00 (3.0716e+00)	Acc@1  43.75 ( 35.63)
 * Acc@1 36.004
after pretraining lr: 0.00017838106725040818
Epoch: [1][   0/1000]	Time  1.24 ( 1.24)	Data  0.01 ( 0.01)	Loss   2.17 (  2.17)	Cls Acc 62.5 (62.5)	Domain Acc 54.7 (54.7)
Epoch: [1][ 100/1000]	Time  1.29 ( 1.29)	Data  0.02 ( 0.03)	Loss   1.74 (  2.01)	Cls Acc 78.1 (73.5)	Domain Acc 75.0 (68.2)
Epoch: [1][ 200/1000]	Time  1.28 ( 1.30)	Data  0.02 ( 0.03)	Loss   1.79 (  2.14)	Cls Acc 75.0 (73.6)	Domain Acc 56.2 (64.2)
Epoch: [1][ 300/1000]	Time  1.28 ( 1.29)	Data  0.02 ( 0.03)	Loss   2.27 (  2.21)	Cls Acc 65.6 (73.2)	Domain Acc 65.6 (60.6)
Epoch: [1][ 400/1000]	Time  1.30 ( 1.29)	Data  0.02 ( 0.03)	Loss   2.38 (  2.19)	Cls Acc 75.0 (72.7)	Domain Acc 40.6 (59.1)
Epoch: [1][ 500/1000]	Time  1.28 ( 1.29)	Data  0.02 ( 0.03)	Loss   1.68 (  2.20)	Cls Acc 71.9 (72.6)	Domain Acc 78.1 (57.9)
Epoch: [1][ 600/1000]	Time  1.28 ( 1.29)	Data  0.02 ( 0.03)	Loss   1.97 (  2.11)	Cls Acc 68.8 (73.1)	Domain Acc 45.3 (59.1)
Traceback (most recent call last):
  File "cloth_bsp.py", line 461, in <module>
    main(args)
  File "cloth_bsp.py", line 279, in main
    train(train_source_iter, train_target_iter, classifier, domain_adv, bsp_penalty, optimizer,
  File "cloth_bsp.py", line 345, in train
    transfer_loss = domain_adv(f_s, f_t)
  File "/home/gtzelepis/miniconda3/envs/domain_adaptation/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/gtzelepis/miniconda3/envs/domain_adaptation/lib/python3.8/site-packages/tllib-0.4-py3.8.egg/tllib/alignment/dann.py", line 76, in forward
    domain_discri = DomainDiscriminator(in_feature=classifier.features_dim, hidden_size=1024).to(device)
KeyboardInterrupt
