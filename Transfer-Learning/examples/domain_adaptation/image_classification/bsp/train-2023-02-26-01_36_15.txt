Namespace(arch='convnext_base', batch_size=32, bottleneck_dim=256, data='Office31', epochs=20, iters_per_epoch=1000, log='bsp', lr=0.003, lr_decay=0.75, lr_gamma=0.001, momentum=0.9, no_hflip=False, no_pool=False, norm_mean=(0.485, 0.456, 0.406), norm_std=(0.229, 0.224, 0.225), per_class_eval=False, phase='train', pretrain=None, pretrain_epochs=3, pretrain_lr=0.001, print_freq=100, ratio=[0.75, 1.3333333333333333], resize_size=224, root='data/office31', scale=[0.08, 1.0], scratch=False, seed=None, source=['A'], target=['W'], trade_off=1.0, trade_off_bsp=0.0002, train_resizing='default', val_resizing='default', weight_decay=0.001, workers=2)
=> using model 'convnext_base'
Pretraining the model on source domain.
/home/gtzelepis/miniconda3/envs/domain_adaptation/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
lr: [0.0001, 0.001, 0.001]
Epoch: [0][   0/1000]	Time 2.8 (2.8)	Data 0.0 (0.0)	Loss 2.31 (2.31)	Cls Acc 3.1 (3.1)
Epoch: [0][ 100/1000]	Time 0.7 (0.7)	Data 0.1 (0.1)	Loss 1.35 (1.62)	Cls Acc 46.9 (44.5)
Epoch: [0][ 200/1000]	Time 0.6 (0.7)	Data 0.1 (0.1)	Loss 1.26 (1.36)	Cls Acc 56.2 (53.6)
Epoch: [0][ 300/1000]	Time 0.6 (0.7)	Data 0.0 (0.1)	Loss 0.85 (1.22)	Cls Acc 78.1 (58.0)
Epoch: [0][ 400/1000]	Time 0.6 (0.7)	Data 0.0 (0.1)	Loss 0.68 (1.13)	Cls Acc 84.4 (60.9)
Epoch: [0][ 500/1000]	Time 0.7 (0.7)	Data 0.1 (0.1)	Loss 0.67 (1.07)	Cls Acc 81.2 (63.2)
Epoch: [0][ 600/1000]	Time 0.8 (0.7)	Data 0.2 (0.1)	Loss 0.73 (1.02)	Cls Acc 75.0 (65.1)
Traceback (most recent call last):
  File "cloth_bsp.py", line 461, in <module>
    main(args)
  File "cloth_bsp.py", line 262, in main
    utils.empirical_risk_minimization(train_source_iter, pretrain_model, pretrain_optimizer,
  File "/home/gtzelepis/Transfer-Learning-Library/examples/domain_adaptation/image_classification/utils.py", line 307, in empirical_risk_minimization
    optimizer.step()
  File "/home/gtzelepis/miniconda3/envs/domain_adaptation/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/gtzelepis/miniconda3/envs/domain_adaptation/lib/python3.8/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/gtzelepis/miniconda3/envs/domain_adaptation/lib/python3.8/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/gtzelepis/miniconda3/envs/domain_adaptation/lib/python3.8/site-packages/torch/optim/sgd.py", line 151, in step
    sgd(params_with_grad,
  File "/home/gtzelepis/miniconda3/envs/domain_adaptation/lib/python3.8/site-packages/torch/optim/sgd.py", line 202, in sgd
    func(params,
  File "/home/gtzelepis/miniconda3/envs/domain_adaptation/lib/python3.8/site-packages/torch/optim/sgd.py", line 241, in _single_tensor_sgd
    d_p = d_p.add(buf, alpha=momentum)
KeyboardInterrupt
